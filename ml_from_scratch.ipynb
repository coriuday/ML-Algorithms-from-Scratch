{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7aee91",
   "metadata": {},
   "source": [
    "# Implement Logistic Regression or Decision Tree without using scikit-learn. Show how the pruning works in the case of DT and Random forest algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import math\n",
    "from models import LogisticRegressionScratch, DecisionTreeScratch, RandomForestScratch, DecisionTreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some simple data for binary classification\n",
    "np.random.seed(0)\n",
    "X = np.array([[0.5, 1.5], [1.0, 1.0], [1.5, 0.5], [3.0, 3.5], [2.0, 2.0], [3.5, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Train our logistic regression model\n",
    "model = LogisticRegressionScratch(lr=0.1, n_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "predictions = model.predict(X)\n",
    "probabilities = model.predict_proba(X)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Actual labels:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3cdeb8",
   "metadata": {},
   "source": [
    "## Decision Tree from Scratch\n",
    "A decision tree classifier using entropy for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d196f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DecisionTreeScratch and DecisionTreeNode classes are now imported from the models.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our decision tree on a simple dataset\n",
    "X_dt = np.array([[0.5, 1.5], [1.0, 1.0], [1.5, 0.5], [3.0, 3.5], [2.0, 2.0], [3.5, 2.5]])\n",
    "y_dt = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Create and train the decision tree\n",
    "dt_model = DecisionTreeScratch(max_depth=3)\n",
    "dt_model.fit(X_dt, y_dt)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.predict(X_dt)\n",
    "\n",
    "print(\"Decision Tree Predictions:\", dt_predictions)\n",
    "print(\"Actual labels:\", y_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f7865",
   "metadata": {},
   "source": [
    "## Decision Tree Pruning\n",
    "Pruning helps prevent overfitting by removing branches that have little predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Function to visualize the tree using networkx\n",
    "def visualize_tree(node, graph=None, parent=None, edge_label=None, node_id=0):\n",
    "    if graph is None:\n",
    "        graph = nx.DiGraph()\n",
    "    label = f\"Leaf: {node.value}\" if node.value is not None else f\"X[{node.feature}] <= {node.threshold:.2f}\"\n",
    "    graph.add_node(node_id, label=label)\n",
    "    if parent is not None:\n",
    "        graph.add_edge(parent, node_id, label=edge_label)\n",
    "    next_id = node_id + 1\n",
    "    if node.left:\n",
    "        next_id = visualize_tree(node.left, graph, node_id, 'True', next_id)\n",
    "    if node.right:\n",
    "        next_id = visualize_tree(node.right, graph, node_id, 'False', next_id)\n",
    "    return next_id if graph is not None else graph\n",
    "\n",
    "# Helper to plot the tree graph\n",
    "def plot_tree_graph(graph):\n",
    "    pos = nx.nx_pydot.graphviz_layout(graph, prog='dot')\n",
    "    labels = nx.get_node_attributes(graph, 'label')\n",
    "    nx.draw(graph, pos, labels=labels, with_labels=True, arrows=True, node_size=2000, node_color='lightblue')\n",
    "    edge_labels = nx.get_edge_attributes(graph, 'label')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "    plt.show()\n",
    "\n",
    "# Pruning function: tries to simplify the tree if it doesn't hurt accuracy\n",
    "def prune_tree(node, X_val, y_val):\n",
    "    if node.value is not None:\n",
    "        return node\n",
    "    node.left = prune_tree(node.left, X_val, y_val)\n",
    "    node.right = prune_tree(node.right, X_val, y_val)\n",
    "    if node.left.value is not None and node.right.value is not None:\n",
    "        leaf_value = Counter(y_val).most_common(1)[0][0]\n",
    "        temp_node = DecisionTreeNode(value=leaf_value)\n",
    "        def predict_with_node(X):\n",
    "            return np.array([leaf_value for _ in X])\n",
    "        def predict_with_subtree(X):\n",
    "            return np.array([DecisionTreeScratch().predict_one(x, node) for x in X])\n",
    "        acc_subtree = np.mean(predict_with_subtree(X_val) == y_val)\n",
    "        acc_leaf = np.mean(predict_with_node(X_val) == y_val)\n",
    "        print(f\"Pruning check: subtree acc={acc_subtree:.2f}, leaf acc={acc_leaf:.2f}\")\n",
    "        if acc_leaf >= acc_subtree:\n",
    "            print(\"Pruned a branch!\")\n",
    "            return temp_node\n",
    "    return node\n",
    "\n",
    "# Visualize tree before pruning\n",
    "G_before = nx.DiGraph()\n",
    "visualize_tree(dt_model.root, G_before)\n",
    "print(\"Tree before pruning:\")\n",
    "plot_tree_graph(G_before)\n",
    "\n",
    "# Prune the tree (using same data for simplicity)\n",
    "dt_model.root = prune_tree(dt_model.root, X_dt, y_dt)\n",
    "\n",
    "# Visualize tree after pruning\n",
    "G_after = nx.DiGraph()\n",
    "visualize_tree(dt_model.root, G_after)\n",
    "print(\"Tree after pruning:\")\n",
    "plot_tree_graph(G_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e439c193",
   "metadata": {},
   "source": [
    "## Random Forest from Scratch\n",
    "Random Forest combines multiple decision trees trained on bootstrapped samples and random feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RandomForestScratch class is now imported from the models.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our random forest on the same dataset\n",
    "rf_model = RandomForestScratch(n_trees=5, max_depth=3)\n",
    "rf_model.fit(X_dt, y_dt)\n",
    "rf_predictions = rf_model.predict(X_dt)\n",
    "\n",
    "print(\"Random Forest Predictions:\", rf_predictions)\n",
    "print(\"Actual labels:\", y_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa03dd",
   "metadata": {},
   "source": [
    "## Next Steps: Streamlit Integration\n",
    "You can now build a user interface and deploy these models in real time using Streamlit."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
